1.bert tmdm model은 5000개의 데이터에 대해서만 작동하기 때문에 다른 여러 영화에 대한 
높은 성능은 기대하기 어려움 

2.다른 데이터를 구해야 했지만 tmdm 데이터셋 처럼 overview와 popularity, vote(현재 bert model 의 라벨)를 
모두 가진 데이터는 찾지 못했음.

3. 이에 적은 데이터를 이용해 라벨링을 해주는 작업을 해보려했고 훈련된 bert model을 이용하여 
unlabeled_data에 대해서 라벨링을 진행함.(overview만 있고 popularity, vote,,는 없는 데이터를 unlabeled_data
로 사용함- data : kaggle의 Movies Plot (Cleaned) data 사용(약 45000개의 영화)

4.라벨링이 끝난 데이터는 이후 tmdm 데이터와 함께 사용될 것이기 때문에 Self_training_Dataset은 먼저 만들었던
TMDM_Dataset과 같은 구조임(model도 똑같음)

5. positive_label을 22000개에 가깝게(Movies Plot (Cleaned) data 의 반) 하는 것이 훈련의 목적이었고
epoch 4 tmdm_model이 16000개로 가장 근접함.

6.완벽하지는 않지만 unlabeled_data를 이용할 수 있는 가능성을 찾은......듯!!
